# -*- coding: utf-8 -*-
"""Microsoft Malware Detection v1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/170i0ii7wEC_-xbD7JxpdFjr749KA5iZa

## Installing missing Packages
"""

!pip install pyunpack patool dask fsspec

"""## Importing Packages"""

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.filterwarnings("ignore")
import shutil
import os
import pandas as pd
import matplotlib
matplotlib.use(u'nbAgg')
# %matplotlib inline
import dask.dataframe as dd
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import seaborn as sns
from tqdm import tqdm
from array import array
import numpy as np
import pickle
from sklearn.manifold import TSNE
from sklearn import preprocessing
import pandas as pd
from multiprocessing import Process# this is used for multithreading
import multiprocessing
import codecs# this is used for file operations 
import random as r
from collections import Counter
from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import log_loss
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import normalize
from sklearn.feature_selection import SelectFromModel

"""## Byte Features

### Importing Data
"""

Y = pd.read_csv("/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/trainLabels.csv")

total = len(Y)*1.
ax = sns.countplot(x = 'Class', data = Y)

for p in ax.patches:
  ax.annotate('{:.1f}%'.format(100*p.get_height()/total), (p.get_x()+0.1, p.get_height()+5))

ax.yaxis.set_ticks(np.linspace(0, total, 11))

ax.set_yticklabels(map('{:.1f}%'.format, 100*ax.yaxis.get_majorticklocs()/total))
plt.show()

"""### Feature Extraction

##### File Size of Byte Files as a feature
"""

byteFiles = '/content/drive/Shareddrives/colab/byteFiles/'

files = os.listdir(byteFiles)
filenames = Y['Id'].tolist()
class_y = Y['Class'].tolist()
class_bytes = []
sizebytes = []
fnames = []

for file in files:
  # os.stat() performs a stat system call on the given path i.e., returns some information regarding the file
  statinfo = os.stat(byteFiles+file)
  file = file.split('.')[0]
  if any(file == filename for filename in filenames):
    i = filenames.index(file)
    class_bytes.append(class_y[i])
    sizebytes.append(statinfo.st_size/(1024.0*1024.0))
    fnames.append(file)

data_size_byte = pd.DataFrame({'ID': fnames, 'Size': sizebytes, 'Class': class_bytes})
print(data_size_byte.head())

ax = sns.boxplot(x = 'Class', y= 'Size', data = data_size_byte)
plt.title('Boxplot of .bytes file sizes')
plt.show()

"""##### Feature Extraction from Byte Files

###### Unigrams

1. Removal of address from each Byte File

2. Convert the Hex Codes to Bag of Words.

3. Unigrams and Bigrams
"""

files = os.listdir(byteFiles)
filenames = []
array = []

for file in files:
  if(file.endswith('bytes')):
    file = file.split('.')[0]
    text_file = open(byteFiles + file + '.txt', 'w+')
    with open(byteFiles + file, "r") as fp:
      lines = ''
      for line in fp:
        a = line.rstrip().split(" ")[1:]
        b = ' '.join(a)
        b = b + '\n'
        text_file.write(b)
      fp.close()
      os.remove(byteFiles + file)
    text_file.close()

files = os.listdir(byteFiles)
filenames2 = []
feature_matrix = np.zeros((len(files), 257), dtype = int)
k = 0

byte_feature_file=open('/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/result.csv','w+')
byte_feature_file.write("ID,0,1,2,3,4,5,6,7,8,9,0a,0b,0c,0d,0e,0f,10,11,12,13,14,15,16,17,18,19,1a,1b,1c,1d,1e,1f,20,21,22,23,24,25,26,27,28,29,2a,2b,2c,2d,2e,2f,30,31,32,33,34,35,36,37,38,39,3a,3b,3c,3d,3e,3f,40,41,42,43,44,45,46,47,48,49,4a,4b,4c,4d,4e,4f,50,51,52,53,54,55,56,57,58,59,5a,5b,5c,5d,5e,5f,60,61,62,63,64,65,66,67,68,69,6a,6b,6c,6d,6e,6f,70,71,72,73,74,75,76,77,78,79,7a,7b,7c,7d,7e,7f,80,81,82,83,84,85,86,87,88,89,8a,8b,8c,8d,8e,8f,90,91,92,93,94,95,96,97,98,99,9a,9b,9c,9d,9e,9f,a0,a1,a2,a3,a4,a5,a6,a7,a8,a9,aa,ab,ac,ad,ae,af,b0,b1,b2,b3,b4,b5,b6,b7,b8,b9,ba,bb,bc,bd,be,bf,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,ca,cb,cc,cd,ce,cf,d0,d1,d2,d3,d4,d5,d6,d7,d8,d9,da,db,dc,dd,de,df,e0,e1,e2,e3,e4,e5,e6,e7,e8,e9,ea,eb,ec,ed,ee,ef,f0,f1,f2,f3,f4,f5,f6,f7,f8,f9,fa,fb,fc,fd,fe,ff,??")
byte_feature_file.write("\n")
for file in files:
    filenames2.append(file)
    byte_feature_file.write(file+",")
    if(file.endswith("txt")):
        with open('byteFiles/'+file,"r") as byte_flie:
            for lines in byte_flie:
                line=lines.rstrip().split(" ")
                for hex_code in line:
                    if hex_code=='??':
                        feature_matrix[k][256]+=1
                    else:
                        feature_matrix[k][int(hex_code,16)]+=1
        byte_flie.close()
    for i, row in enumerate(feature_matrix[k]):
        if i!=len(feature_matrix[k])-1:
            byte_feature_file.write(str(row)+",")
        else:
            byte_feature_file.write(str(row))
    byte_feature_file.write("\n")
    k += 1

byte_feature_file.close()

byte_features = pd.read_csv('/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/AAIC Data/result.csv')
byte_features['ID']  = byte_features['ID'].str.split('.').str[0]
byte_features.head(2)

data_size_byte.head(2)

byte_features_with_size = byte_features.merge(data_size_byte, on = 'ID')
byte_features_with_size.to_csv("/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/unigram_with_size.csv")
byte_features_with_size.head(2)

byte_features_with_size = pd.read_csv("/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/unigram_with_size.csv")

# Normalizing Columns
def normalize(df):
  result_copy = df.copy()
  for feature_name in df.columns:
    if(str(feature_name) != str('ID') and str(feature_name) != str('Class')):
      max_value = df[feature_name].max()
      min_value = df[feature_name].min()
      result_copy[feature_name] = (df[feature_name] - min_value)/(max_value - min_value)
  return result_copy

result = normalize(byte_features_with_size)

data_y = result['Class']
result.head()

"""###### Bigrams

1. Create vocab: String with all possible bigram combinations. This will be used as column heading as well. 

2. Depending on the number of cores available, divide the dataset into x number of parts. Each part will be processed by one core. 

3. Each process will parse the byte file, 
"""

hexadecimal_alphabet = list("0123456789abcdef")
vocab = []
for i in hexadecimal_alphabet:
  for j in hexadecimal_alphabet:
    vocab.append(i+j)
vocab = list(set(vocab))
vocab.append("??")
vocab.sort()

vocab_string = ','.join(vocab)
print(vocab_string)

bigrams_list = []
for i in vocab:
  for j in vocab:
    bigrams_list.append(i+"_"+j)
bigrams_list = sorted(bigrams_list)
bigrams_string = ",".join(bigrams_list)

def bigrams_from_line(text):
  """
  This function takes a line and returns all bigrams in that line. 
  https://stackoverflow.com/questions/21844546/forming-bigrams-of-words-in-list-of-sentences-with-python
  """
  bigrams_in_line = [bigram for bigram in zip(text.split(" ")[:-1], text.split(" ")[1:])]
  bigrams_in_line = ["_".join(bigram) for bigram in bigrams_in_line]
  return bigrams_in_line

def bigrams_from_file(loc, bigrams_list):
  """
  This function takes the path to a file and returns a counter with all bigrams counts
  """
  with open(byteFiles + loc, 'r') as byteFile:
    counter = Counter()
    counter.update({x:0 for x in bigrams_list})
    for line in byteFile:
      line_lowercase = line.rstrip().lower()
      line_bigrams = bigrams_from_line(line_lowercase)
      counter.update(line_bigrams)
  byteFile.close()
  return counter

!lscpu | grep 'Core(s) per socket:'

def singleprocess():
  files = os.listdir(byteFiles)
  output_file = open("/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/bigrams_singleprocess.csv", "w+")
  output_file.write("ID,"+bigrams_string+"\n")
  for file in tqdm(files):
    output_file.write(file.split('.')[0]+",")
    counter = bigrams_from_file(file, bigrams_list)
    line_bigrams = [pair[1] for pair in sorted(counter.items())]
    line_bigrams_str = ','.join(str(i) for i in line_bigrams)
    output_file.write(line_bigrams_str+"\n")
  output_file.close()

singleprocess()

"""###### Selecting top 2500 bigrams"""

bigrams = pd.read_csv("/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/bigrams_singleprocess.csv", dtype="int32", converters = {'ID':str})

bigrams.head()

bigrams.sort_values('ID', ignore_index=True, inplace=True)

Y.sort_values('Id', ignore_index=True, inplace = True)

X_train, X_test, y_train, y_test  = train_test_split(bigrams, Y, test_size = 0.3)

X_train.head()

X_train.drop("ID", inplace=True, axis = 1)
X_test.drop("ID", inplace=True, axis = 1)
y_train.drop("Id", inplace=True, axis = 1)
y_test.drop("Id", inplace=True, axis = 1)

clf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)
clf.fit(X_train, y_train)

sfm = SelectFromModel(clf, max_features = 2500)
sfm.fit(X_train, y_train)

sfm.get_support(indices=True)

important_bigrams = [X_train.columns[i] for i in sfm.get_support(indices=True)]
len(important_bigrams)

bigrams_only_important = bigrams[important_bigrams]

bigrams_only_important["ID"] = bigrams["ID"]
bigrams_only_important.head()

bigrams_only_important.to_csv("/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/important_bigrams.csv")

"""###### Importing Bigrams CSV"""

from google.colab import drive
drive.mount('/content/drive')

from sklearn.preprocessing import normalize

bigrams = pd.read_csv("/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/important_bigrams.csv", dtype="int32", converters = {'ID':str, 'Size':float})

byte_features = bigrams.merge(result, on = "ID")

from sklearn.preprocessing import normalize

byte_features_no_id = byte_features.drop(["ID", "Class"], axis = 1)
byte_features_no_id_norm = normalize(byte_features_no_id)
bigram_column_names = byte_features_no_id.columns
byte_features_norm = pd.DataFrame(data=byte_features_no_id_norm, columns = bigram_column_names)
byte_features_ids = byte_features["ID"]
byte_features_classes = byte_features["Class"]
byte_features_norm.insert(loc = 0, column = "ID", value = byte_features_ids)
byte_features_norm.insert(loc = 0, column = "Class", value = byte_features_classes)
y = byte_features_norm["Class"]
byte_features_norm.drop("Unnamed: 0_x", axis = 1, inplace =True)
byte_features_norm.drop("Class", inplace=True, axis = 1)
byte_features_norm.drop("ID", inplace = True, axis = 1)
byte_features_norm.head()

"""### Multivariate Analysis

#### T-SNE to check whether the features made so far using the Byte Files are helpful or not in classifying malware.
"""

xtsne = TSNE(perplexity=50)
results = xtsne.fit_transform(byte_features_norm)

vis_x = results[:, 0]
vis_y = results[:, 1]
plt.scatter(vis_x, vis_y, c = data_y, cmap = plt.cm.get_cmap('jet', 9))
plt.colorbar(ticks = range(10))
plt.clim(0.5, 9)
plt.show()

xtsne=TSNE(perplexity=30)
results=xtsne.fit_transform(result.drop(['ID','Class'], axis=1))

vis_x = results[:, 0]
vis_y = results[:, 1]
plt.scatter(vis_x, vis_y, c=data_y, cmap=plt.cm.get_cmap("jet", 9))
plt.colorbar(ticks=range(10))
plt.clim(0.5, 9)
plt.show()

"""TSNE is able to seperate them to some extent. This shows that the results are quite useful.

#### Train CV Test Split

Random split since the dataset is not of temporal nature.
"""

X_train, X_test, y_train, y_test = train_test_split(byte_features_norm, y ,stratify=y,test_size=0.20)
X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train,stratify=y_train,test_size=0.20)

print(f"Number of datapoints in train data: {X_train.shape[0]}")
print(f"Number of datapoints in cross validation data: {X_cv.shape[0]}")
print(f"Number of datapoints in test data: {X_test.shape[0]}")

"""#### Plotting the Class Distribution among the three datasets. """

train_class_distribution = y_train.value_counts().sort_index()
test_class_distribution = y_test.value_counts().sort_index()
cv_class_distribution = y_cv.value_counts().sort_index()

plot_colors = list('rgbkymc')
train_class_distribution.plot(kind = 'bar', color = plot_colors)
plt.xlabel('Class')
plt.ylabel('Data Points per Class')
plt.title('Distribution of yi in train data')
plt.grid()
plt.show()

sorted_yi = np.argsort(-train_class_distribution.values)
for i in sorted_yi:
    print('Number of data points in class', i+1, ':',train_class_distribution.values[i], '(', np.round((train_class_distribution.values[i]/y_train.shape[0]*100), 3), '%)')

plot_colors = list('rgbkymc')
cv_class_distribution.plot(kind = 'bar', color = plot_colors)
plt.xlabel('Class')
plt.ylabel('Data Points per Class')
plt.title('Distribution of yi in CV data')
plt.grid()
plt.show()

sorted_yi = np.argsort(-cv_class_distribution.values)
for i in sorted_yi:
    print('Number of data points in class', i+1, ':',cv_class_distribution.values[i], '(', np.round((cv_class_distribution.values[i]/y_cv.shape[0]*100), 3), '%)')

plot_colors = list('rgbkymc')
test_class_distribution.plot(kind = 'bar', color = plot_colors)
plt.xlabel('Class')
plt.ylabel('Data Points per Class')
plt.title('Distribution of yi in Test data')
plt.grid()
plt.show()

sorted_yi = np.argsort(-test_class_distribution.values)
for i in sorted_yi:
    print('Number of data points in class', i+1, ':',test_class_distribution.values[i], '(', np.round((test_class_distribution.values[i]/y_test.shape[0]*100), 3), '%)')

def plot_confusion_matrix(test_y, predict_y):
  C = confusion_matrix(test_y, predict_y)
  print(f"Number of misclassified points: {(len(test_y)-np.trace(C))/len(test_y)*100}")
  # Recall Matrix
  A = (((C.T)/(C.sum(axis=1))).T)
  # Precision Matrix
  B = (C/C.sum(axis=0))
  
  labels = [1,2,3,4,5,6,7,8,9]
  cmap = sns.light_palette('green')

  print('-'*50, 'Confusion Matrix','-'*50)
  plt.figure(figsize=(10,5))
  sns.heatmap(C, annot=True, cmap = cmap, fmt = '.3f', xticklabels = labels, yticklabels = labels)
  plt.xlabel('Predicted Class')
  plt.ylabel('Original Class')
  plt.show()

  print('-'*50, 'Precision Matrix','-'*50)
  plt.figure(figsize=(10,5))
  sns.heatmap(B, annot=True, cmap=cmap, fmt=".3f", xticklabels=labels, yticklabels=labels)
  plt.xlabel('Predicted Class')
  plt.ylabel('Original Class')
  plt.show()
  print(f"Sum of columns in precision matrix {B.sum(axis=0)}")

  print("-"*50, "Recall matrix"    , "-"*50)
  plt.figure(figsize=(10,5))
  sns.heatmap(A, annot=True, cmap=cmap, fmt=".3f", xticklabels=labels, yticklabels=labels)
  plt.xlabel('Predicted Class')
  plt.ylabel('Original Class')
  plt.show()
  print(f"Sum of rows in precision matrix {A.sum(axis=1)}")

"""### Machine Learning Models

##### Machine Learning Models on Byte Files

#### Random Model

Generate 9 numbers and their sum should be 1.
"""

test_data_len = X_test.shape[0]
cv_data_len = X_cv.shape[0]

cv_predicted_y = np.zeros((cv_data_len, 9))
for i in range(cv_data_len):
  rand_probs = np.random.rand(1,9)
  cv_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])
print(f"Log Loss on Cross Validation Data using Random Model: {log_loss(y_cv, cv_predicted_y, eps=1e-15)}")

test_predicted_y = np.zeros((test_data_len, 9))
for i in range(test_data_len):
  rand_probs = np.random.rand(1,9)
  test_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])
print(f"Log Loss on Test Data using Random Model: {log_loss(y_test, test_predicted_y, eps = 1e-15)}")

predicted_y = np.argmax(test_predicted_y, axis = 1)
plot_confusion_matrix(y_test, predicted_y+1)

"""#### K Nearest Neighbour Classification"""

alpha = [x for x in range(1, 15, 2)]

cv_log_error_array = []

for i in alpha:
  k_cfl = KNeighborsClassifier(n_neighbors = i)
  k_cfl.fit(X_train, y_train)
  sig_clf = CalibratedClassifierCV(k_cfl, method = 'sigmoid')
  sig_clf.fit(X_train, y_train)
  predict_y = sig_clf.predict_proba(X_cv)
  cv_log_error_array.append(log_loss(y_cv, predict_y, labels = k_cfl.classes_, eps = 1e-15))

for i in range(len(cv_log_error_array)):
  print(f"Log Loss for k  = {alpha[i]} is {cv_log_error_array[i]}")

best_alpha = np.argmin(cv_log_error_array)

fig, ax = plt.subplots()
ax.plot(alpha, cv_log_error_array, c = 'g')
for i, txt in enumerate(np.round(cv_log_error_array, 3)):
  ax.annotate((alpha[i], np.round(txt, 3)), (alpha[i], cv_log_error_array[i]))

plt.grid()
plt.title("Cross Validation Error for each alpha")
plt.xlabel("Alpha i's")
plt.ylabel("Error Measure")
plt.show()

k_cfl = KNeighborsClassifier(n_neighbors = alpha[best_alpha])
k_cfl.fit(X_train, y_train)
sig_clf = CalibratedClassifierCV(k_cfl, method='sigmoid')
sig_clf.fit(X_train, y_train)

predict_y = sig_clf.predict_proba(X_train)
print(f"For value of best alpha: {alpha[best_alpha]}, the train log loss is {log_loss(y_train, predict_y)}")
predict_y = sig_clf.predict_proba(X_cv)
print(f"For value of best alpha: {alpha[best_alpha]}, the cv log loss is {log_loss(y_cv, predict_y)}")
predict_y = sig_clf.predict_proba(X_test)
print(f"For value of best alpha: {alpha[best_alpha]}, the test log loss is {log_loss(y_test, predict_y)}")

plot_confusion_matrix(y_test, sig_clf.predict(X_test))

"""#### Logistic Regression"""

alpha = [10 ** x for x in range(-5, 4)]
cv_log_error_array=[]
for i in alpha:
    logisticR=LogisticRegression(penalty='l2',C=i,class_weight='balanced')
    logisticR.fit(X_train,y_train)
    sig_clf = CalibratedClassifierCV(logisticR, method="sigmoid")
    sig_clf.fit(X_train, y_train)
    predict_y = sig_clf.predict_proba(X_cv)
    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=logisticR.classes_, eps=1e-15))
    
for i in range(len(cv_log_error_array)):
    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])

best_alpha = np.argmin(cv_log_error_array)
    
fig, ax = plt.subplots()
ax.plot(alpha, cv_log_error_array,c='g')
for i, txt in enumerate(np.round(cv_log_error_array,3)):
    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))
plt.grid()
plt.title("Cross Validation Error for each alpha")
plt.xlabel("Alpha i's")
plt.ylabel("Error measure")
plt.show()

logisticR=LogisticRegression(penalty='l2',C=alpha[best_alpha],class_weight='balanced')
logisticR.fit(X_train,y_train)
sig_clf = CalibratedClassifierCV(logisticR, method="sigmoid")
sig_clf.fit(X_train, y_train)
pred_y=sig_clf.predict(X_test)

predict_y = sig_clf.predict_proba(X_train)
print ('log loss for train data',log_loss(y_train, predict_y, labels=logisticR.classes_, eps=1e-15))
predict_y = sig_clf.predict_proba(X_cv)
print ('log loss for cv data',log_loss(y_cv, predict_y, labels=logisticR.classes_, eps=1e-15))
predict_y = sig_clf.predict_proba(X_test)
print ('log loss for test data',log_loss(y_test, predict_y, labels=logisticR.classes_, eps=1e-15))
plot_confusion_matrix(y_test, sig_clf.predict(X_test))

"""#### XGBoost"""

alpha=[10,50,100,500,1000,2000]
cv_log_error_array=[]
for i in alpha:
    x_cfl=XGBClassifier(n_estimators=i,nthread=-1)
    x_cfl.fit(X_train,y_train)
    sig_clf = CalibratedClassifierCV(x_cfl, method="sigmoid")
    sig_clf.fit(X_train, y_train)
    predict_y = sig_clf.predict_proba(X_cv)
    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=x_cfl.classes_, eps=1e-15))

for i in range(len(cv_log_error_array)):
    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])


best_alpha = np.argmin(cv_log_error_array)

fig, ax = plt.subplots()
ax.plot(alpha, cv_log_error_array,c='g')
for i, txt in enumerate(np.round(cv_log_error_array,3)):
    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))
plt.grid()
plt.title("Cross Validation Error for each alpha")
plt.xlabel("Alpha i's")
plt.ylabel("Error measure")
plt.show()

x_cfl=XGBClassifier(n_estimators=alpha[best_alpha],nthread=-1)
x_cfl.fit(X_train,y_train)
sig_clf = CalibratedClassifierCV(x_cfl, method="sigmoid")
sig_clf.fit(X_train, y_train)
    
predict_y = sig_clf.predict_proba(X_train)
print ('For values of best alpha = ', alpha[best_alpha], "The train log loss is:",log_loss(y_train, predict_y))
predict_y = sig_clf.predict_proba(X_cv)
print('For values of best alpha = ', alpha[best_alpha], "The cross validation log loss is:",log_loss(y_cv, predict_y))
predict_y = sig_clf.predict_proba(X_test)
print('For values of best alpha = ', alpha[best_alpha], "The test log loss is:",log_loss(y_test, predict_y))
plot_confusion_matrix(y_test, sig_clf.predict(X_test))

"""#### Saving and Loading XGBoost Models"""

pickle.dump(x_cfl, open("/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/byte_features_xgboost.pkl","wb"))
pickle.dump(sig_clf, open("/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/byte_features_callibratedclassifer.pkl","wb"))

sig_clf = pickle.load(open("/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/byte_features_callibratedclassifier.pkl","rb"))

alpha=[10,50,100,500,1000,2000]
best_alpha = 4

predict_y = sig_clf.predict_proba(X_train)
print ('For values of best alpha = ', alpha[best_alpha], "The train log loss is:",log_loss(y_train, predict_y))
predict_y = sig_clf.predict_proba(X_cv)
print('For values of best alpha = ', alpha[best_alpha], "The cross validation log loss is:",log_loss(y_cv, predict_y))
predict_y = sig_clf.predict_proba(X_test)
print('For values of best alpha = ', alpha[best_alpha], "The test log loss is:",log_loss(y_test, predict_y))
plot_confusion_matrix(y_test, sig_clf.predict(X_test))

"""## ASM Features

### Feature Extraction from ASM Files

#### OPCode Features

There is about 150GB of Data that needs to be processed.
"""

folder_1 ='first'
folder_2 ='second'
folder_3 ='third'
folder_4 ='fourth'
folder_5 ='fifth'
folder_6 = 'output'
for i in [folder_1,folder_2,folder_3,folder_4,folder_5,folder_6]:
    if not os.path.isdir(i):
        os.makedirs(i)

source='/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/train'
files = os.listdir('train')
data=range(0,10868)
r.shuffle(data)
count=0
for i in range(0,10868):
    if i % 5==0:
        shutil.move(source+files[data[i]],'first')
    elif i%5==1:
        shutil.move(source+files[data[i]],'second')
    elif i%5 ==2:
        shutil.move(source+files[data[i]],'third')
    elif i%5 ==3:
        shutil.move(source+files[data[i]],'fourth')
    elif i%5==4:
        shutil.move(source+files[data[i]],'fifth')

#http://flint.cs.yale.edu/cs421/papers/x86-asm/asm.html
def firstprocess():
    #The prefixes tells about the segments that are present in the asm files
    #There are 450 segments(approx) present in all asm files.
    #this prefixes are best segments that gives us best values.
    
    prefixes = ['HEADER:','.text:','.Pav:','.idata:','.data:','.bss:','.rdata:','.edata:','.rsrc:','.tls:','.reloc:','.BSS:','.CODE']
    #this are opcodes that are used to get best results
    #https://en.wikipedia.org/wiki/X86_instruction_listings
    
    opcodes = ['jmp', 'mov', 'retf', 'push', 'pop', 'xor', 'retn', 'nop', 'sub', 'inc', 'dec', 'add','imul', 'xchg', 'or', 'shr', 'cmp', 'call', 'shl', 'ror', 'rol', 'jnb','jz','rtn','lea','movzx']
    #best keywords that are taken from different blogs
    keywords = ['.dll','std::',':dword']
    #Below taken registers are general purpose registers and special registers
    #All the registers which are taken are best 
    registers=['edx','esi','eax','ebx','ecx','edi','ebp','esp','eip']
    file1=open("output\asmsmallfile.txt","w+")
    files = os.listdir('first')
    for f in files:
        #filling the values with zeros into the arrays
        prefixescount=np.zeros(len(prefixes),dtype=int)
        opcodescount=np.zeros(len(opcodes),dtype=int)
        keywordcount=np.zeros(len(keywords),dtype=int)
        registerscount=np.zeros(len(registers),dtype=int)
        features=[]
        f2=f.split('.')[0]
        file1.write(f2+",")
        opcodefile.write(f2+" ")
        # https://docs.python.org/3/library/codecs.html#codecs.ignore_errors
        # https://docs.python.org/3/library/codecs.html#codecs.Codec.encode
        with codecs.open('first/'+f,encoding='cp1252',errors ='replace') as fli:
            for lines in fli:
                # https://www.tutorialspoint.com/python3/string_rstrip.htm
                line=lines.rstrip().split()
                l=line[0]
                #counting the prefixs in each and every line
                for i in range(len(prefixes)):
                    if prefixes[i] in line[0]:
                        prefixescount[i]+=1
                line=line[1:]
                #counting the opcodes in each and every line
                for i in range(len(opcodes)):
                    if any(opcodes[i]==li for li in line):
                        features.append(opcodes[i])
                        opcodescount[i]+=1
                #counting registers in the line
                for i in range(len(registers)):
                    for li in line:
                        # we will use registers only in 'text' and 'CODE' segments
                        if registers[i] in li and ('text' in l or 'CODE' in l):
                            registerscount[i]+=1
                #counting keywords in the line
                for i in range(len(keywords)):
                    for li in line:
                        if keywords[i] in li:
                            keywordcount[i]+=1
        #pushing the values into the file after reading whole file
        for prefix in prefixescount:
            file1.write(str(prefix)+",")
        for opcode in opcodescount:
            file1.write(str(opcode)+",")
        for register in registerscount:
            file1.write(str(register)+",")
        for key in keywordcount:
            file1.write(str(key)+",")
        file1.write("\n")
    file1.close()

def secondprocess():
    prefixes = ['HEADER:','.text:','.Pav:','.idata:','.data:','.bss:','.rdata:','.edata:','.rsrc:','.tls:','.reloc:','.BSS:','.CODE']
    opcodes = ['jmp', 'mov', 'retf', 'push', 'pop', 'xor', 'retn', 'nop', 'sub', 'inc', 'dec', 'add','imul', 'xchg', 'or', 'shr', 'cmp', 'call', 'shl', 'ror', 'rol', 'jnb','jz','rtn','lea','movzx']
    keywords = ['.dll','std::',':dword']
    registers=['edx','esi','eax','ebx','ecx','edi','ebp','esp','eip']
    file1=open("output\mediumasmfile.txt","w+")
    files = os.listdir('second')
    for f in files:
        prefixescount=np.zeros(len(prefixes),dtype=int)
        opcodescount=np.zeros(len(opcodes),dtype=int)
        keywordcount=np.zeros(len(keywords),dtype=int)
        registerscount=np.zeros(len(registers),dtype=int)
        features=[]
        f2=f.split('.')[0]
        file1.write(f2+",")
        opcodefile.write(f2+" ")
        with codecs.open('second/'+f,encoding='cp1252',errors ='replace') as fli:
            for lines in fli:
                line=lines.rstrip().split()
                l=line[0]
                for i in range(len(prefixes)):
                    if prefixes[i] in line[0]:
                        prefixescount[i]+=1
                line=line[1:]
                for i in range(len(opcodes)):
                    if any(opcodes[i]==li for li in line):
                        features.append(opcodes[i])
                        opcodescount[i]+=1
                for i in range(len(registers)):
                    for li in line:
                        if registers[i] in li and ('text' in l or 'CODE' in l):
                            registerscount[i]+=1
                for i in range(len(keywords)):
                    for li in line:
                        if keywords[i] in li:
                            keywordcount[i]+=1
        for prefix in prefixescount:
            file1.write(str(prefix)+",")
        for opcode in opcodescount:
            file1.write(str(opcode)+",")
        for register in registerscount:
            file1.write(str(register)+",")
        for key in keywordcount:
            file1.write(str(key)+",")
        file1.write("\n")
    file1.close()

def thirdprocess():
    prefixes = ['HEADER:','.text:','.Pav:','.idata:','.data:','.bss:','.rdata:','.edata:','.rsrc:','.tls:','.reloc:','.BSS:','.CODE']
    opcodes = ['jmp', 'mov', 'retf', 'push', 'pop', 'xor', 'retn', 'nop', 'sub', 'inc', 'dec', 'add','imul', 'xchg', 'or', 'shr', 'cmp', 'call', 'shl', 'ror', 'rol', 'jnb','jz','rtn','lea','movzx']
    keywords = ['.dll','std::',':dword']
    registers=['edx','esi','eax','ebx','ecx','edi','ebp','esp','eip']
    file1=open("output\largeasmfile.txt","w+")
    files = os.listdir('third')
    for f in files:
        prefixescount=np.zeros(len(prefixes),dtype=int)
        opcodescount=np.zeros(len(opcodes),dtype=int)
        keywordcount=np.zeros(len(keywords),dtype=int)
        registerscount=np.zeros(len(registers),dtype=int)
        features=[]
        f2=f.split('.')[0]
        file1.write(f2+",")
        opcodefile.write(f2+" ")
        with codecs.open('third/'+f,encoding='cp1252',errors ='replace') as fli:
            for lines in fli:
                line=lines.rstrip().split()
                l=line[0]
                for i in range(len(prefixes)):
                    if prefixes[i] in line[0]:
                        prefixescount[i]+=1
                line=line[1:]
                for i in range(len(opcodes)):
                    if any(opcodes[i]==li for li in line):
                        features.append(opcodes[i])
                        opcodescount[i]+=1
                for i in range(len(registers)):
                    for li in line:
                        if registers[i] in li and ('text' in l or 'CODE' in l):
                            registerscount[i]+=1
                for i in range(len(keywords)):
                    for li in line:
                        if keywords[i] in li:
                            keywordcount[i]+=1
        for prefix in prefixescount:
            file1.write(str(prefix)+",")
        for opcode in opcodescount:
            file1.write(str(opcode)+",")
        for register in registerscount:
            file1.write(str(register)+",")
        for key in keywordcount:
            file1.write(str(key)+",")
        file1.write("\n")
    file1.close()

def fourthprocess():
    prefixes = ['HEADER:','.text:','.Pav:','.idata:','.data:','.bss:','.rdata:','.edata:','.rsrc:','.tls:','.reloc:','.BSS:','.CODE']
    opcodes = ['jmp', 'mov', 'retf', 'push', 'pop', 'xor', 'retn', 'nop', 'sub', 'inc', 'dec', 'add','imul', 'xchg', 'or', 'shr', 'cmp', 'call', 'shl', 'ror', 'rol', 'jnb','jz','rtn','lea','movzx']
    keywords = ['.dll','std::',':dword']
    registers=['edx','esi','eax','ebx','ecx','edi','ebp','esp','eip']
    file1=open("output\hugeasmfile.txt","w+")
    files = os.listdir('fourth/')
    for f in files:
        prefixescount=np.zeros(len(prefixes),dtype=int)
        opcodescount=np.zeros(len(opcodes),dtype=int)
        keywordcount=np.zeros(len(keywords),dtype=int)
        registerscount=np.zeros(len(registers),dtype=int)
        features=[]
        f2=f.split('.')[0]
        file1.write(f2+",")
        opcodefile.write(f2+" ")
        with codecs.open('fourth/'+f,encoding='cp1252',errors ='replace') as fli:
            for lines in fli:
                line=lines.rstrip().split()
                l=line[0]
                for i in range(len(prefixes)):
                    if prefixes[i] in line[0]:
                        prefixescount[i]+=1
                line=line[1:]
                for i in range(len(opcodes)):
                    if any(opcodes[i]==li for li in line):
                        features.append(opcodes[i])
                        opcodescount[i]+=1
                for i in range(len(registers)):
                    for li in line:
                        if registers[i] in li and ('text' in l or 'CODE' in l):
                            registerscount[i]+=1
                for i in range(len(keywords)):
                    for li in line:
                        if keywords[i] in li:
                            keywordcount[i]+=1
        for prefix in prefixescount:
            file1.write(str(prefix)+",")
        for opcode in opcodescount:
            file1.write(str(opcode)+",")
        for register in registerscount:
            file1.write(str(register)+",")
        for key in keywordcount:
            file1.write(str(key)+",")
        file1.write("\n")
    file1.close()

def fifthprocess():
    prefixes = ['HEADER:','.text:','.Pav:','.idata:','.data:','.bss:','.rdata:','.edata:','.rsrc:','.tls:','.reloc:','.BSS:','.CODE']
    opcodes = ['jmp', 'mov', 'retf', 'push', 'pop', 'xor', 'retn', 'nop', 'sub', 'inc', 'dec', 'add','imul', 'xchg', 'or', 'shr', 'cmp', 'call', 'shl', 'ror', 'rol', 'jnb','jz','rtn','lea','movzx']
    keywords = ['.dll','std::',':dword']
    registers=['edx','esi','eax','ebx','ecx','edi','ebp','esp','eip']
    file1=open("output\trainasmfile.txt","w+")
    files = os.listdir('fifth/')
    for f in files:
        prefixescount=np.zeros(len(prefixes),dtype=int)
        opcodescount=np.zeros(len(opcodes),dtype=int)
        keywordcount=np.zeros(len(keywords),dtype=int)
        registerscount=np.zeros(len(registers),dtype=int)
        features=[]
        f2=f.split('.')[0]
        file1.write(f2+",")
        opcodefile.write(f2+" ")
        with codecs.open('fifth/'+f,encoding='cp1252',errors ='replace') as fli:
            for lines in fli:
                line=lines.rstrip().split()
                l=line[0]
                for i in range(len(prefixes)):
                    if prefixes[i] in line[0]:
                        prefixescount[i]+=1
                line=line[1:]
                for i in range(len(opcodes)):
                    if any(opcodes[i]==li for li in line):
                        features.append(opcodes[i])
                        opcodescount[i]+=1
                for i in range(len(registers)):
                    for li in line:
                        if registers[i] in li and ('text' in l or 'CODE' in l):
                            registerscount[i]+=1
                for i in range(len(keywords)):
                    for li in line:
                        if keywords[i] in li:
                            keywordcount[i]+=1
        for prefix in prefixescount:
            file1.write(str(prefix)+",")
        for opcode in opcodescount:
            file1.write(str(opcode)+",")
        for register in registerscount:
            file1.write(str(register)+",")
        for key in keywordcount:
            file1.write(str(key)+",")
        file1.write("\n")
    file1.close()


def main():
    manager=multiprocessing.Manager() 	
    p1=Process(target=firstprocess)
    p2=Process(target=secondprocess)
    p3=Process(target=thirdprocess)
    p4=Process(target=fourthprocess)
    p5=Process(target=fifthprocess)
    #p1.start() is used to start the thread execution
    p1.start()
    p2.start()
    p3.start()
    p4.start()
    p5.start()
    #After completion all the threads are joined
    p1.join()
    p2.join()
    p3.join()
    p4.join()
    p5.join()

if __name__=="__main__":
    main()

dfasm = pd.read_csv("/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/AAIC Data/asmoutputfile.csv")
Y.columns = ['ID','Class']
result_asm = pd.merge(dfasm, Y, on = 'ID', how = 'left')
result_asm.head()

"""#### File Size Feature"""

asmFiles = '/content/drive/Shareddrives/colab/asmFiles/'

files = os.listdir(asmFiles)
filenames = Y['ID'].tolist()
class_Y = Y['Class'].tolist()
class_bytes = []
sizebytes = []
fnames = []

for file in files:
  statinfo = os.stat(asmFiles + file)
  file = file.split('.')[0]
  if any(file == filename for filename in filenames):
    i = filenames.index(file)
    class_bytes.append(class_Y[i])
    sizebytes.append(statinfo.st_size/(1024.0*1024.0))
    fnames.append(file)
asm_size_byte = pd.DataFrame({'ID':fnames, 'Size':sizebytes, 'Class':class_bytes})
asm_size_byte.head()

"""##### Distribution of .asm file sizes"""

ax = sns.boxplot(x="Class", y="Size", data=asm_size_byte)
plt.title("Boxplot of .bytes file sizes")
plt.show()

# Adding File Size Feature to Previous Extracted Features
print(result_asm.shape)
print(asm_size_byte.shape)
result_asm = pd.merge(result_asm, asm_size_byte.drop(['Class'], axis=1),on='ID', how='left')
result_asm.head()

# Normalizing Columns
def normalize(df):
  result_copy = df.copy()
  for feature_name in df.columns:
    if(str(feature_name) != str('ID') and str(feature_name) != str('Class')):
      max_value = df[feature_name].max()
      min_value = df[feature_name].min()
      result_copy[feature_name] = (df[feature_name] - min_value)/(max_value - min_value)
  return result_copy

result_asm = normalize(result_asm)
result_asm.head()

"""##### Univariate Analysis on .asm file features"""

ax = sns.boxplot(x="Class", y=".text:", data=result_asm)
plt.title("Boxplot of .asm Text Segment")
plt.show()

ax = sns.boxplot(x="Class", y=".Pav:", data=result_asm)
plt.title("Boxplot of .asm pav Segment")
plt.show()

ax = sns.boxplot(x="Class", y=".data:", data=result_asm)
plt.title("Boxplot of .asm data segment")
plt.show()

ax = sns.boxplot(x="Class", y=".bss:", data=result_asm)
plt.title("Boxplot of .asm bss Segment")
plt.show()

ax = sns.boxplot(x="Class", y=".rdata:", data=result_asm)
plt.title("Boxplot of .asm rdata Segment")
plt.show()

ax = sns.boxplot(x="Class", y="jmp", data=result_asm)
plt.title("Boxplot of .asm jmp opcode")
plt.show()

ax = sns.boxplot(x="Class", y="mov", data=result_asm)
plt.title("Boxplot of .asm mov opcode")
plt.show()

ax = sns.boxplot(x="Class", y="mov", data=result_asm)
plt.title("Boxplot of .asm mov opcode")
plt.show()

ax = sns.boxplot(x="Class", y="push", data=result_asm)
plt.title("Boxplot of .asm push opcode")
plt.show()

"""#### Pixel Density Feature"""

import array
# def get_800_pixel(source):
#     if source.endswith(".asm"):
#         file=open(source,"rb")
#         ln=os.path.getsize(source)
#         width=int(ln*0.5)
#         rem=ln%width
#         a=array.array("B")# unit8 array
#         a.fromfile(file,ln-rem)
#         file.close()
#         return np.array(list(a[:800]))
pxl_colmns=["ID"]+["pxl_"+str(i) for i in range(800)]  
def get_800_pixel(source):
    if source.endswith(".asm"):
        source="asmFiles/"+source
        file=open(source,"rb")
        ln=os.path.getsize(source)
        width=int(ln*0.5)
        rem=ln%width
        a=array.array("B")# unit8 array
        a.fromfile(file,ln-rem)
        file.close()
        g=np.reshape(a,(int(len(a)/width),width))
        g=np.uint8(g)
        #print(800-len(g[0][:300]))
        if len(g[0])>800:
            return np.array(g[0][:800])
        else:
            return np.pad(g[0],(0,800-len(g[0])),mode="constant",constant_values=(0,0))
    
files = os.listdir('asmFiles')
pxl_intensity=[]
for file in tqdm(files):
    temp=get_800_pixel(file)
    temp=np.concatenate([[file.split(".")[0]],temp])
#     print(temp)
    pxl_intensity.append(temp)

pxl_intensity_df=pd.DataFrame(pxl_intensity,columns=pxl_colmns)

def pixel_density(file):
  """
  http://sarvamblog.blogspot.ca/2014/08/supervised-classification-with-k-fold.html
  Padding: https://stackoverflow.com/questions/45422000/add-n-zeros-to-the-end-of-an-array
  800 asm features: https://www.kaggle.com/c/malware-classification/discussion/13490
  """
  f = open(file,"rb")
  ln = os.path.getsize(file)
  width = int(ln**0.5)
  rem = ln%width
  a = array("B")
  a.fromfile(f, ln-rem)
  f.close()
  g = np.reshape(a,(int(len(a)/width),width))
  g = np.uint8(g)
  if g.shape[0] > 800:
    return g[0][:800]
  else:
    # in case shape is less that 800, we add zeros at the end and return it
    return np.pad(g[0], (0, 800-g[0].shape[0]), 'constant', constant_values=(0,0))

files = os.listdir(asmFiles)
pixel_densities = []

for file in tqdm(files):
  file_density = pixel_density(asmFiles+file)
  density_list = file_density.tolist()
  density_list.append(file.split(".")[0])
  pixel_densities.append(density_list)

cols = ["pixel_"+str(i) for i in range(800)]
cols.append("ID")

pixel_densities_df = pd.DataFrame(pixel_densities, columns = cols)

pixel_densities_df.head()

pixel_densities_df.to_csv("/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/pixel_densities.csv", index=False)

asm_features = result_asm.merge(pixel_densities_df, on = "ID")

asm_features = asm_features.merge(asm_size_byte, on = "ID")

asm_features.head()

asm_features = asm_features.rename(columns={"Size_y":"Size","Class_y":"Class"})

asm_features.to_csv("/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/asm_features.csv", index=False)

"""### Multivariate Analysis on .asm file features"""

xtsne=TSNE(perplexity=50)
results=xtsne.fit_transform(asm_features.drop(['ID','Class'], axis=1).fillna(0))
vis_x = results[:, 0]
vis_y = results[:, 1   ]
plt.scatter(vis_x, vis_y, c=data_y, cmap=plt.cm.get_cmap("jet", 9))
plt.colorbar(ticks=range(10))
plt.clim(0.5, 9)
plt.show()

xtsne=TSNE(perplexity=30)
results=xtsne.fit_transform(asm_features.drop(['ID','Class', 'rtn', '.BSS:', '.CODE','Size'], axis=1))
vis_x = results[:, 0]
vis_y = results[:, 1]
plt.scatter(vis_x, vis_y, c=data_y, cmap=plt.cm.get_cmap("jet", 9))
plt.colorbar(ticks=range(10))
plt.clim(0.5, 9)
plt.show()

"""### Train Test Split"""

asm_features = pd.read_csv("/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/asm_features.csv")

asm_y = asm_features['Class']
asm_x = asm_features.drop(['ID','Class','.BSS:','rtn','.CODE'], axis=1)

X_train_asm, X_test_asm, y_train_asm, y_test_asm = train_test_split(asm_x,asm_y ,stratify=asm_y,test_size=0.20)
X_train_asm, X_cv_asm, y_train_asm, y_cv_asm = train_test_split(X_train_asm, y_train_asm,stratify=y_train_asm,test_size=0.20)

print( X_cv_asm.isnull().all())

"""### Machine Learning Models on .asm Files

#### K Nearest Neighbours
"""

alpha = [x for x in range(1, 21,2)]
cv_log_error_array=[]
for i in tqdm(alpha):
    k_cfl=KNeighborsClassifier(n_neighbors=i)
    k_cfl.fit(X_train_asm,y_train_asm)
    sig_clf = CalibratedClassifierCV(k_cfl, method="sigmoid")
    sig_clf.fit(X_train_asm, y_train_asm)
    predict_y = sig_clf.predict_proba(X_cv_asm)
    cv_log_error_array.append(log_loss(y_cv_asm, predict_y, labels=k_cfl.classes_, eps=1e-15))
    
for i in range(len(cv_log_error_array)):
    print ('log_loss for k = ',alpha[i],'is',cv_log_error_array[i])

best_alpha = np.argmin(cv_log_error_array)
    
fig, ax = plt.subplots()
ax.plot(alpha, cv_log_error_array,c='g')
for i, txt in enumerate(np.round(cv_log_error_array,3)):
    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))
plt.grid()
plt.title("Cross Validation Error for each alpha")
plt.xlabel("Alpha i's")
plt.ylabel("Error measure")
plt.show()

k_cfl=KNeighborsClassifier(n_neighbors=alpha[best_alpha])
k_cfl.fit(X_train_asm,y_train_asm)
sig_clf = CalibratedClassifierCV(k_cfl, method="sigmoid")
sig_clf.fit(X_train_asm, y_train_asm)
pred_y=sig_clf.predict(X_test_asm)


predict_y = sig_clf.predict_proba(X_train_asm)
print ('log loss for train data',log_loss(y_train_asm, predict_y))
predict_y = sig_clf.predict_proba(X_cv_asm)
print ('log loss for cv data',log_loss(y_cv_asm, predict_y))
predict_y = sig_clf.predict_proba(X_test_asm)
print ('log loss for test data',log_loss(y_test_asm, predict_y))
plot_confusion_matrix(y_test_asm,sig_clf.predict(X_test_asm))

"""#### Logistic Regression"""

alpha = [10 ** x for x in range(-5, 4)]
cv_log_error_array=[]
for i in tqdm(alpha):
    logisticR=LogisticRegression(penalty='l2',C=i,class_weight='balanced')
    logisticR.fit(X_train_asm,y_train_asm)
    sig_clf = CalibratedClassifierCV(logisticR, method="sigmoid")
    sig_clf.fit(X_train_asm, y_train_asm)
    predict_y = sig_clf.predict_proba(X_cv_asm)
    cv_log_error_array.append(log_loss(y_cv_asm, predict_y, labels=logisticR.classes_, eps=1e-15))
    
for i in range(len(cv_log_error_array)):
    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])

best_alpha = np.argmin(cv_log_error_array)
    
fig, ax = plt.subplots()
ax.plot(alpha, cv_log_error_array,c='g')
for i, txt in enumerate(np.round(cv_log_error_array,3)):
    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))
plt.grid()
plt.title("Cross Validation Error for each alpha")
plt.xlabel("Alpha i's")
plt.ylabel("Error measure")
plt.show()

logisticR=LogisticRegression(penalty='l2',C=alpha[best_alpha],class_weight='balanced')
logisticR.fit(X_train_asm,y_train_asm)
sig_clf = CalibratedClassifierCV(logisticR, method="sigmoid")
sig_clf.fit(X_train_asm, y_train_asm)

predict_y = sig_clf.predict_proba(X_train_asm)
print ('log loss for train data',(log_loss(y_train_asm, predict_y, labels=logisticR.classes_, eps=1e-15)))
predict_y = sig_clf.predict_proba(X_cv_asm)
print ('log loss for cv data',(log_loss(y_cv_asm, predict_y, labels=logisticR.classes_, eps=1e-15)))
predict_y = sig_clf.predict_proba(X_test_asm)
print ('log loss for test data',(log_loss(y_test_asm, predict_y, labels=logisticR.classes_, eps=1e-15)))
plot_confusion_matrix(y_test_asm,sig_clf.predict(X_test_asm))

"""#### Random Forest Classifier"""

alpha=[10,50,100,500,1000,2000,3000]
cv_log_error_array=[]
for i in tqdm(alpha):
    r_cfl=RandomForestClassifier(n_estimators=i,random_state=42,n_jobs=-1)
    r_cfl.fit(X_train_asm,y_train_asm)
    sig_clf = CalibratedClassifierCV(r_cfl, method="sigmoid")
    sig_clf.fit(X_train_asm, y_train_asm)
    predict_y = sig_clf.predict_proba(X_cv_asm)
    cv_log_error_array.append(log_loss(y_cv_asm, predict_y, labels=r_cfl.classes_, eps=1e-15))

for i in range(len(cv_log_error_array)):
    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])


best_alpha = np.argmin(cv_log_error_array)

fig, ax = plt.subplots()
ax.plot(alpha, cv_log_error_array,c='g')
for i, txt in enumerate(np.round(cv_log_error_array,3)):
    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))
plt.grid()
plt.title("Cross Validation Error for each alpha")
plt.xlabel("Alpha i's")
plt.ylabel("Error measure")
plt.show()

r_cfl=RandomForestClassifier(n_estimators=alpha[best_alpha],random_state=42,n_jobs=-1)
r_cfl.fit(X_train_asm,y_train_asm)
sig_clf = CalibratedClassifierCV(r_cfl, method="sigmoid")
sig_clf.fit(X_train_asm, y_train_asm)
predict_y = sig_clf.predict_proba(X_train_asm)
print ('log loss for train data',(log_loss(y_train_asm, predict_y, labels=sig_clf.classes_, eps=1e-15)))
predict_y = sig_clf.predict_proba(X_cv_asm)
print ('log loss for cv data',(log_loss(y_cv_asm, predict_y, labels=sig_clf.classes_, eps=1e-15)))
predict_y = sig_clf.predict_proba(X_test_asm)
print ('log loss for test data',(log_loss(y_test_asm, predict_y, labels=sig_clf.classes_, eps=1e-15)))
plot_confusion_matrix(y_test_asm,sig_clf.predict(X_test_asm))

"""#### XGBoost Classifier"""

alpha=[10,50,100,500,1000,2000,3000]
cv_log_error_array=[]
for i in tqdm(alpha):
    x_cfl=XGBClassifier(n_estimators=i,nthread=-1)
    x_cfl.fit(X_train_asm,y_train_asm)
    sig_clf = CalibratedClassifierCV(x_cfl, method="sigmoid")
    sig_clf.fit(X_train_asm, y_train_asm)
    predict_y = sig_clf.predict_proba(X_cv_asm)
    cv_log_error_array.append(log_loss(y_cv_asm, predict_y, labels=x_cfl.classes_, eps=1e-15))

for i in range(len(cv_log_error_array)):
    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])


best_alpha = np.argmin(cv_log_error_array)

fig, ax = plt.subplots()
ax.plot(alpha, cv_log_error_array,c='g')
for i, txt in enumerate(np.round(cv_log_error_array,3)):
    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))
plt.grid()
plt.title("Cross Validation Error for each alpha")
plt.xlabel("Alpha i's")
plt.ylabel("Error measure")
plt.show()

x_cfl=XGBClassifier(n_estimators=alpha[best_alpha],nthread=-1)
x_cfl.fit(X_train_asm,y_train_asm)
sig_clf = CalibratedClassifierCV(x_cfl, method="sigmoid")
sig_clf.fit(X_train_asm, y_train_asm)
    
predict_y = sig_clf.predict_proba(X_train_asm)

print ('For values of best alpha = ', alpha[best_alpha], "The train log loss is:",log_loss(y_train_asm, predict_y))
predict_y = sig_clf.predict_proba(X_cv_asm)
print('For values of best alpha = ', alpha[best_alpha], "The cross validation log loss is:",log_loss(y_cv_asm, predict_y))
predict_y = sig_clf.predict_proba(X_test_asm)
print('For values of best alpha = ', alpha[best_alpha], "The test log loss is:",log_loss(y_test_asm, predict_y))
plot_confusion_matrix(y_test_asm,sig_clf.predict(X_test_asm))

pickle.dump(x_cfl, open("/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/asm_features_xgboost.pkl","wb"))
pickle.dump(sig_clf, open("/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/asm_features_callibratedclassifer.pkl","wb"))

"""## Combining byte and asm features

### Importing Data
"""

bigrams = pd.read_csv("/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/important_bigrams.csv", dtype="int32", converters = {'ID':str, 'Size':float})
byte_features = bigrams.merge(result, on = "ID")

byte_features_no_id = byte_features.drop(["ID", "Class"], axis = 1)
byte_features_no_id_norm = normalize(byte_features_no_id)
bigram_column_names = byte_features_no_id.columns
byte_features_norm = pd.DataFrame(data=byte_features_no_id_norm, columns = bigram_column_names)
byte_features_ids = byte_features["ID"]
byte_features_norm.insert(loc = 0, column = "ID", value = byte_features_ids)
byte_features_norm.drop("Unnamed: 0_x", axis = 1, inplace =True)
byte_features_norm.head()

byte_features_norm = byte_features_norm.rename(columns={"Size":"Byte_Size"})

asm_features = asm_features.rename(columns={"Size":"asm_Size"})

final_df = byte_features_norm.merge(asm_features, on = "ID")

final_df.head()

final_df.to_csv("/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/final_features.csv")

final_df = pd.read_csv("/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/final_features.csv")

"""### Train Test Split"""

final_y = final_df['Class']
final_x = final_df.drop(['ID','Class','.BSS:','rtn','.CODE'], axis=1)

"""### Modelling on Byte and asm Features"""

X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(final_x,final_y ,stratify=final_y,test_size=0.20)
X_train_final, X_cv_final, y_train_final, y_cv_final = train_test_split(X_train_final, y_train_final,stratify=y_train_final,test_size=0.20)

alpha=[10,50,100,500,1000,2000,3000]
cv_log_error_array=[]
for i in tqdm(alpha):
    x_cfl=XGBClassifier(n_estimators=i,nthread=-1)
    x_cfl.fit(X_train_final,y_train_final)
    sig_clf = CalibratedClassifierCV(x_cfl, method="sigmoid")
    sig_clf.fit(X_train_final, y_train_final)
    predict_y = sig_clf.predict_proba(X_cv_final)
    temp_log_loss = log_loss(y_cv_final, predict_y, labels=x_cfl.classes_, eps=1e-15)
    cv_log_error_array.append(temp_log_loss)
    print(f"\nLog Loss for {i} trees is {temp_log_loss}")


for i in range(len(cv_log_error_array)):
    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])


best_alpha = np.argmin(cv_log_error_array)

fig, ax = plt.subplots()
ax.plot(alpha, cv_log_error_array,c='g')
for i, txt in enumerate(np.round(cv_log_error_array,3)):
    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))
plt.grid()
plt.title("Cross Validation Error for each alpha")
plt.xlabel("Alpha i's")
plt.ylabel("Error measure")
plt.show()

x_cfl=XGBClassifier(n_estimators=alpha[best_alpha],nthread=-1)
x_cfl.fit(X_train_final,y_train_final)
sig_clf = CalibratedClassifierCV(x_cfl, method="sigmoid")
sig_clf.fit(X_train_final, y_train_final)
    
predict_y = sig_clf.predict_proba(X_train_final)

print ('For values of best alpha = ', alpha[best_alpha], "The train log loss is:",log_loss(y_train_final, predict_y))
predict_y = sig_clf.predict_proba(X_cv_final)
print('For values of best alpha = ', alpha[best_alpha], "The cross validation log loss is:",log_loss(y_cv_final, predict_y))
predict_y = sig_clf.predict_proba(X_test_final)
print('For values of best alpha = ', alpha[best_alpha], "The test log loss is:",log_loss(y_test_final, predict_y))
plot_confusion_matrix(y_test_final,sig_clf.predict(X_test_final))

pickle.dump(x_cfl, open("/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/final_features_xgboost.pkl","wb"))
pickle.dump(sig_clf, open("/content/drive/MyDrive/AAIC/Case Studies/Microsoft Malware Detection/final_features_callibratedclassifer.pkl","wb"))

"""## Results Table"""

train_results = [2.452, 0.072, 0.498, 0.026, 0.022, 2.492, 0.098, 0.884, 0.018, 0.047, 0.396, 0.011, 0.011, 0.021, 0.582, 0.009, 0.007, 0.008]
cv_results =    [2.483, 0.225, 0.549, 0.087, 0.093, 2.439, 0.289, 0.866, 0.034, 0.095, 0.424, 0.049, 0.056, 0.071, 0.575, 0.188, 0.011, 0.009]
test_results =  [2.485, 0.241, 0.528, 0.085, 0.079, 2.872, 0.327, 0.887, 0.029, 0.089, 0.415, 0.057, 0.046, 0.062, 0.578, 0.022, 0.009, 0.014]
dataset_names = ['Unigrams + Byte Size', 'Unigrams + Byte Size', 'Unigrams + Byte Size', 'Unigrams + Byte Size', 'Unigrams + Byte Size', 'Unigrams + Bigrams + Byte Size', 'Unigrams + Bigrams + Byte Size', 'Unigrams + Bigrams + Byte Size', 'Unigrams + Bigrams + Byte Size', 'Op Code + asm Size','Op Code + asm Size','Op Code + asm Size','Op Code + asm Size', 'Op Code + asm Size + Pixel Density', 'Op Code + asm Size + Pixel Density', 'Op Code + asm Size + Pixel Density', 'Op Code + asm Size + Pixel Density', 'All Byte + asm Features']
model_names = ['Random Model', 'KNN', 'Logistic Regression', 'Random Forest', 'XGBoost', 'Random Model', 'KNN', 'Logistic Regression', 'XGBoost', 'KNN', 'Logistic Regression', 'Random Forest', 'XGBoost', 'KNN', 'Logistic Regression', 'Random Forest', 'XGBoost', 'XGBoost']
colors = ['rgb(89, 193, 115)', 'rgb(89, 193, 115)', 'rgb(89, 193, 115)', 'rgb(89, 193, 115)', 'rgb(89, 193, 115)', 'rgb(119, 94, 166)', 'rgb(119, 94, 166)', 'rgb(119, 94, 166)', 'rgb(119, 94, 166)', 'rgb(161, 127, 224)', 'rgb(161, 127, 224)', 'rgb(161, 127, 224)', 'rgb(161, 127, 224)', 'rgb(119, 94, 166)', 'rgb(119, 94, 166)', 'rgb(119, 94, 166)', 'rgb(119, 94, 166)', 'rgb(93, 38, 193)',]
results_data = {'Dataset': dataset_names, 'Model': model_names, 'Train Log Loss': train_results, 'CV Log Loss': cv_results, 'Test Log Loss': test_results, "Color": colors}
results_df = pd.DataFrame(results_data)

fig = go.Figure(data=[go.Table(
  header=dict(
    values=["Dataset", "Model", "Train Log Loss", "CV Log Loss", "Test Log Loss"],
    line_color='white', fill_color='white',
    align='center', font=dict(color='black', size=12)
  ),
  cells=dict(
    values=[results_df.Dataset, results_df.Model, results_df["Train Log Loss"], results_df["CV Log Loss"], results_df["Test Log Loss"]],
    line_color='black', fill_color=[results_df.Color],
    align='center', font=dict(color='black', size=11)
  ))
])

fig.show()